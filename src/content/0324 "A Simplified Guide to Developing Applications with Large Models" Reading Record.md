---
title_en: 0324 "A Simplified Guide to Developing Applications with Large Models" Reading Record
title: 0324 《大模型应用开发极简入门》阅读记录
date: 2023-03-24
---

- 基础术语速览
	- 注意力机制，Attention Mechanism
		- 交叉注意力，Cross-Attention

			交叉注意力机制，是一种**使模型能够关注输入序列的不同部分以便更好地进行预测的技术。**在自然语言处理中，这种机制使模型能够在生成输出的每个步骤时，将注意力分配到输入序列的所有位置，尤其是那些对当前预测最相关的位置。这能够帮助模型理解语言中的复杂模式和依赖关系。例如，在机器翻译任务中，交叉注意力可以帮助模型理解源语言和目标语言之间的对齐关系。

		- 自注意力，Self-Attention

			自注意力机制是一种在模型的输入序列中，**让模型自行决定应该关注哪些部分以便更好地进行预测的技术**。在自注意力机制中，模型会为输入序列中的每个单元（例如，单词或字符）分配一个权重，这个权重反映了该单元在生成输出时的重要性。这种机制允许模型捕捉到输入序列中的长距离依赖关系，并且能够在并行计算环境中高效地运行。自注意力机制是 Transformer 模型的核心组成部分，被广泛应用于各种自然语言处理任务中。

	- 思维链，Chain of Thought
		- 一种提示工程技术，核心思想是通过向大语言模型输入少量的示例，在示例中将具体问题拆分成多个推理步骤，并要求模型遵循步骤逐步思考。这会改善模型在执行复杂的推理任务时的表现。
	- 嵌入，embedding
		- 表示词语或者句子且能被机器学习模型处理的实值向量。对相似的数据进行向量化，在信息检索等任务中，嵌入的能力非常有用。
	- 少样本学习，Few-shot learning
		- 对模型输入少量示例输入输出，通过如此操作训练模型学习，处理期望的任务的技术。
	- 微调，fine-tuning
		- 对预训练模型进行再次训练，通过输入一个特定较小的数据集进一步训练模型，微调目的在重复使用与训练模型的特征，并使其适应特定的任务。微调一般对模型结构不会产生变化，仅仅更新了模型的权重参数。
	- 基础模型，foundation model
		- 基础模型是在大量未标记的数据上进行训练的，这类模型可以执行各种任务，如图像分析和文本翻译。基础模型的关键特点是能通过无监督学习从原始数据中学习，并通过微调来执行特定的任务。
	- 函数调用，function call
		- 通过对大模型输出进行特定的结构化设定，使大模型输出结果能识别成外部函数的调用交互的协议，再将调用结果结构化返回给大模型作为输入。通过该能力使得大模型能够和外部调用工具进行协作。
	- 生成式预训练，Transformer Generative Pre-trained Transformer,  GPT
		- 基于 Transformer 架构，在大量文本数据的基础上进行训练出来的模型。这类模型能通过迭代地预测序列中的下一个单词来生成连贯且与上下文相关的句子。
	- 推理，Inference
		- 使用训练过的机器学习模型进行预测和判断的过程。
	- 语言模型，Language model
		- 用于自然语言处理的人工智能模型，能够阅读和生成人类语言。语言模型是对词序列的概率分布，通过训练文本数据来学习一门语言的结构和模式。大语言模型是具有大量参数的语言模型，经过大量文本语料库训练。它能够生成自然语言文本和处理复杂语境下的问题。
	- 长短期记忆，Long short-term memory，LSTM
		- 一种用户处理序列数据中短期及长期依赖关系的循环神经网络架构。基于 Transformer 的 LLM，不再使用 LSTM，而是使用注意力机制。
	- 多模态模型，Multimodal model
		- 能够处理和融合多种数据的模型。这些数据可以包括文本、图像、音频视频等不同模态的数据。
	- n-gram 算法
		- 常用于根据词频预测字符串中的下一个单词。这是一种早期自然语言处理 （natural language processing，NLP）中常用的文本补全算法。
	- 参数，Parameter
		- 参数是大语言模型的权重，在训练阶段，模型根据模型创建者选择的优化策略来优化这些系数。参数量是模型大小和复杂性的衡量标准。
	- 预训练，Pre-trained
		- 机器学习模型在大型和通用的数据集上进行的初始训练阶段。对于新任务，预训练模型可以针对该任务进行微调。
	- 提示词，Prompt
		- 输入给语言模型的内容，模型通过它生成输出。
	- 提示工程，Prompt engineering
		- 设计和优化提示词，以从语言模型中获得所需的输出。提示工程可以包括，示例输入输出，思考步骤的设计等等，让模型能更好地进行推理思考并获得预期的输出。
	- 提示词注入，Prompt injection
		- 通过巧妙设计的提示词，让大模型行为脱离原先设定的任务。
	- 循环神经网络，Recurrent neural network，RNN
		- 一类表现出时间动态行为的神经网络，适应于涉及序列数据的任务，例如文本序列或时间序列。
	- 强化学习，Reinforcement learning，RL
		- 一种机器学习方法，专注于在环境中训练模型以最大化奖励反馈，模型利用反馈并进一步学习和改进。
	- 通过人类反馈进行强化学习，Reinforcement learning from human feedback，RLHF
		- 一种将强化学习和人类反馈相结合的训练模型的技术，该技术使用人类反馈来创建奖励信号，用该信号进行强化学习来训练改进模型。
	- 监督微调， Supervised fine-tuning，SFT
		- 采用预训练好的神经网络模型，并针对特定任务或领域在少量的监督数据上对齐进行重新训练。
	- 监督学习，Supervised learning
		- 一种机器学习方法，通过经过人工标记的高质量的训练数据来训练模型，使得模型从训练数据中习得一种模式，以达到期望的行为，如准确预测结果和目标分类。
	- 迁移学习，Transfer learning
		- 一种机器学习方式，将在一个任务上训练的模型重复运用于另一个相关任务上。例如对经过大量数据预训练的大语言模型进行较少的数据训练微调，使其运用于特定的任务上。
	- Transformer 架构，Transformer architecture
		- 一种常用于自然语言处理任务的神经网络架构。它基于自注意力机制，可并行处理数据，其效率比 RNN 和 LSTM 要高效。
	- 无监督学习，Unsupervised learning
		- 一种机器学习方式，它使用机器学习算法来分析处理数据集，对数据集进行标记分类，采用该方法能使模型无需人工干预即可习得特定的模式和能力。
	- 零样本学习，Zero-shot learning
		- 在大语言模型中，其对在训练期间没有明确见过的数据和情况进行预测的行为。通过在提示词中直接给定任务，无示例等信息，使得大模型直接根据预训练的数据进行回答。
